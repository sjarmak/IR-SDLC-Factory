{"task_type": "stack_trace_to_fix", "repo_name": "grafana/grafana", "title": "Nil pointer panic in alerting notification", "scenario": "\nProduction alert: Grafana crashes intermittently when sending alert notifications.\n\nStack trace:\n```\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1]\n\ngoroutine 847 [running]:\npkg/services/ngalert/notifier.(*Alertmanager).SendNotification(...)\n    /app/pkg/services/ngalert/notifier/alertmanager.go:234\npkg/services/ngalert/schedule.(*schedule).notify(...)\n    /app/pkg/services/ngalert/schedule/schedule.go:445\n```\n\nThis happens ~2% of the time, always during high-load periods.\n", "vague_prompt": "Grafana keeps crashing when sending alerts. Fix it.", "ground_truth": {"root_cause": "Contact point can be nil when notification policy references deleted receiver", "root_cause_location": "pkg/services/ngalert/notifier/alertmanager.go", "fix_approach": "Add nil check before dereferencing contact point, log warning for missing receiver", "related_code": ["pkg/services/ngalert/notifier/alertmanager.go", "pkg/services/ngalert/api/api_receiver.go", "pkg/services/ngalert/models/contact_point.go"]}, "scoring_criteria": {"correct_root_cause": {"weight": 0.4, "levels": {"exact": "Identifies nil contact point from deleted receiver", "partial": "Identifies nil dereference but wrong cause", "wrong": "Blames unrelated code"}}, "fix_correctness": {"weight": 0.4, "levels": {"correct": "Adds nil check + handles gracefully", "partial": "Adds check but silent failure", "wrong": "Doesn't fix the issue"}}, "explanation": {"weight": 0.2, "levels": {"educational": "Explains race condition clearly", "adequate": "Explains fix without context"}}}, "difficulty": "hard", "tags": ["debugging", "panic", "nil-pointer", "concurrency"], "task_id": "grafana__grafana-stack_trace_to_fix-794b0190f90b", "category": "debugging", "created_at": "2025-12-29T18:16:22.945514+00:00"}
{"task_type": "flaky_test_diagnosis", "repo_name": "kubernetes/kubernetes", "title": "Pod scheduling test flakes on CI", "scenario": "\nThis test fails approximately 5-10% of CI runs:\n\n```\n=== FAIL: TestScheduler_SchedulePod (0.23s)\n    scheduler_test.go:445: expected pod to be scheduled within 5s, got timeout\n    scheduler_test.go:446: pod status: Pending\n```\n\nThe test passes consistently on developer machines.\nCI runs on smaller instances with 2 CPUs.\n\nTest code summary:\n- Creates a scheduler with default config\n- Submits a pod\n- Waits 5 seconds for scheduling\n- Asserts pod is scheduled\n", "vague_prompt": "This scheduling test keeps failing on CI. It works on my machine.", "ground_truth": {"root_cause": "Race condition: test doesn't wait for informer cache sync before scheduling", "contributing_factors": ["CI has slower I/O than dev machines", "5s timeout too short for cold cache", "Missing WaitForCacheSync call"], "fix_approach": "Add informer cache sync wait before scheduling, increase timeout with CI detection", "related_code": ["pkg/scheduler/scheduler_test.go", "pkg/scheduler/scheduler.go", "staging/src/k8s.io/client-go/tools/cache/shared_informer.go"]}, "scoring_criteria": {"identifies_race_condition": {"weight": 0.4, "levels": {"exact": "Identifies informer cache sync issue", "partial": "Identifies timing issue but wrong component", "wrong": "Blames unrelated factor"}}, "fix_quality": {"weight": 0.4, "levels": {"correct": "Adds cache sync + appropriate timeout", "partial": "Just increases timeout", "wrong": "Doesn't address the race"}}}, "difficulty": "expert", "tags": ["testing", "flaky", "race-condition", "kubernetes"], "task_id": "kubernetes__kubernetes-flaky_test_diagnosis-253eaac588d0", "category": "debugging", "created_at": "2025-12-29T18:16:22.945663+00:00"}
{"task_type": "log_analysis", "repo_name": "elastic/elasticsearch", "title": "Cluster yellow status investigation", "scenario": "\nProduction cluster showing yellow status. Relevant log snippets:\n\n```\n[2024-01-15T14:32:01] [WARN] [cluster.routing.allocation.decider] \n  [node-3] high disk watermark [90%] exceeded on [node-3], \n  shards will be relocated away from this node\n\n[2024-01-15T14:32:45] [INFO] [cluster.routing.allocation] \n  Rerouting 24 shards from [node-3] to other nodes\n\n[2024-01-15T14:33:12] [WARN] [cluster.routing.allocation]\n  unable to allocate shard [logs-2024.01.15][2]: no eligible nodes\n\n[2024-01-15T14:33:15] [WARN] [cluster.health]\n  Cluster status changed from [GREEN] to [YELLOW]\n```\n\nThe ops team wants to know: What happened? Will it recover? What should we do?\n", "vague_prompt": "Elasticsearch cluster went yellow. Why?", "ground_truth": {"root_cause": "Disk watermark exceeded on node-3, triggering shard relocation, but other nodes also near capacity", "chain_of_events": ["node-3 exceeded 90% disk", "ES started relocating 24 shards", "No other node had capacity for some shards", "Unallocated shards caused yellow status"], "will_recover": "Maybe - depends on disk space on other nodes", "recommended_actions": ["Add disk capacity to cluster", "Delete old indices to free space", "Adjust watermark thresholds temporarily"]}, "scoring_criteria": {"correct_diagnosis": {"weight": 0.4, "levels": {"complete": "Identifies full chain of events", "partial": "Identifies disk issue but not cascade"}}, "actionable_advice": {"weight": 0.4, "levels": {"complete": "Multiple options with trade-offs", "partial": "Single recommendation"}}, "recovery_prediction": {"weight": 0.2, "levels": {"correct": "Conditional based on cluster state", "partial": "Generic answer"}}}, "difficulty": "hard", "tags": ["debugging", "log-analysis", "elasticsearch", "ops"], "task_id": "elastic__elasticsearch-log_analysis-130021d00d23", "category": "debugging", "created_at": "2025-12-29T18:16:22.945667+00:00"}
{"task_type": "api_migration", "repo_name": "grafana/grafana", "title": "Migrate from deprecated context API", "scenario": "\nGrafana's codebase uses a deprecated context pattern in many places:\n\nOld pattern (deprecated):\n```go\nfunc (s *Service) DoSomething(ctx *models.ReqContext) error {\n    user := ctx.SignedInUser\n    orgID := ctx.OrgId\n    // ...\n}\n```\n\nNew pattern (required):\n```go\nfunc (s *Service) DoSomething(ctx context.Context) error {\n    user := appcontext.User(ctx)\n    orgID := identity.OrgIDFromContext(ctx)\n    // ...\n}\n```\n\nMigrate the alerting service (pkg/services/ngalert/) to use the new pattern.\nThere are approximately 45 usages across 12 files.\n", "vague_prompt": "Update the alerting service to use the new context API", "ground_truth": {"files_to_change": 12, "usages_to_migrate": 45, "key_changes": ["Replace *models.ReqContext with context.Context", "Use appcontext.User(ctx) for user access", "Use identity.OrgIDFromContext(ctx) for org ID", "Update callers to construct proper context"], "pitfalls": ["Some code paths need both old and new context during transition", "Test fixtures need updating", "HTTP handlers need middleware changes"]}, "scoring_criteria": {"completeness": {"weight": 0.4, "levels": {"all": "All 45 usages migrated correctly", "most": ">80% migrated", "some": ">50% migrated", "few": "<50% migrated"}}, "correctness": {"weight": 0.4, "levels": {"correct": "All tests pass, behavior unchanged", "partial": "Minor issues in edge cases", "broken": "Behavior changed or tests fail"}}, "idiomatic": {"weight": 0.2, "levels": {"clean": "Follows Go context best practices", "functional": "Works but not idiomatic"}}}, "difficulty": "hard", "tags": ["migration", "api-change", "go", "context"], "task_id": "grafana__grafana-api_migration-aaa02bc80274", "category": "code_transformation", "created_at": "2025-12-29T18:16:22.945669+00:00"}
{"task_type": "merge_conflict_resolution", "repo_name": "microsoft/vscode", "title": "Resolve extension activation conflicts", "scenario": "\nTwo branches have conflicting changes to extension activation:\n\nBranch `main`:\n- Refactored extension host to use async activation\n- Changed ExtensionHostManager.activate() signature\n- Added activation timing telemetry\n\nBranch `feature-eager-activation`:  \n- Added eager activation for workspace-trust-sensitive extensions\n- Modified activation order logic\n- Added new activation event types\n\nMerge conflicts in:\n- src/vs/workbench/services/extensions/common/extensionHostManager.ts\n- src/vs/workbench/services/extensions/common/extensionActivation.ts\n\nBoth changes are valuable and should be preserved.\n", "vague_prompt": "Merge the eager-activation feature into main", "ground_truth": {"conflict_resolution": ["Keep async activation from main", "Keep eager activation logic from feature branch", "Combine activation event types", "Ensure timing telemetry covers eager activations"], "semantic_merges": ["ExtensionHostManager.activate() needs both async + eager handling", "Activation order must respect both async and eager constraints"], "test_requirements": ["Existing async activation tests pass", "New eager activation behavior works", "No activation order regressions"]}, "scoring_criteria": {"both_features_work": {"weight": 0.5, "levels": {"yes": "Async and eager activation both work", "partial": "One feature compromised", "no": "One or both features broken"}}, "no_regressions": {"weight": 0.3, "levels": {"clean": "All existing tests pass", "minor": "Some test adjustments needed", "major": "Significant test failures"}}, "code_quality": {"weight": 0.2, "levels": {"clean": "Merged code is maintainable", "messy": "Works but needs cleanup"}}}, "difficulty": "expert", "tags": ["merge-conflict", "refactoring", "typescript"], "task_id": "microsoft__vscode-merge_conflict_resolution-c18760cdc907", "category": "code_transformation", "created_at": "2025-12-29T18:16:22.945671+00:00"}
{"task_type": "build_failure_diagnosis", "repo_name": "kubernetes/kubernetes", "title": "Cross-compilation failure for arm64", "scenario": "\nCI build for arm64 is failing:\n\n```\n# Building for linux/arm64\ngo build -o _output/bin/kube-apiserver ./cmd/kube-apiserver\n# k8s.io/kubernetes/vendor/go.etcd.io/bbolt\nvendor/go.etcd.io/bbolt/bolt_arm64.go:8:2: undefined: maxMapSize\nvendor/go.etcd.io/bbolt/bolt_arm64.go:9:2: undefined: maxAllocSize\nFAIL\n```\n\nThis started failing after a recent dependency update.\nThe amd64 build works fine.\n", "vague_prompt": "ARM64 build is broken. Fix it.", "ground_truth": {"root_cause": "bbolt version incompatible with current Go version for arm64", "diagnosis_steps": ["Check recent changes to go.mod", "Identify bbolt version change", "Check bbolt release notes for arm64 issues"], "fix_options": ["Pin bbolt to compatible version", "Upgrade Go to version with fix", "Apply patch for arm64 constants"]}, "scoring_criteria": {"correct_diagnosis": {"weight": 0.4, "levels": {"exact": "Identifies bbolt + Go version incompatibility", "partial": "Identifies dependency issue", "wrong": "Blames unrelated component"}}, "fix_works": {"weight": 0.4, "levels": {"yes": "Build passes on arm64", "partial": "Build passes but other issues", "no": "Still fails"}}, "minimal_change": {"weight": 0.2, "levels": {"minimal": "Only changes necessary deps", "acceptable": "Some extra changes", "excessive": "Too many unrelated changes"}}}, "difficulty": "hard", "tags": ["ci", "build-failure", "cross-compilation", "arm64"], "task_id": "kubernetes__kubernetes-build_failure_diagnosis-aa719ed9a7ee", "category": "ci_cd", "created_at": "2025-12-29T18:16:22.945673+00:00"}
{"task_type": "dependency_conflict_resolution", "repo_name": "grafana/grafana", "title": "React peer dependency conflicts", "scenario": "\nAfter updating to a new charting library, npm install fails:\n\n```\nnpm ERR! ERESOLVE unable to resolve dependency tree\nnpm ERR! \nnpm ERR! peer react@\"^17.0.0\" from @visx/visx@2.18.0\nnpm ERR! node_modules/@visx/visx\nnpm ERR!   @visx/visx@\"^2.18.0\" from the root project\nnpm ERR! \nnpm ERR! Could not resolve dependency:\nnpm ERR! peer react@\"^17.0.0 || ^18.0.0\" from react-dom@18.2.0\nnpm ERR! node_modules/react-dom\nnpm ERR!   react-dom@\"^18.2.0\" from the root project\nnpm ERR!\nnpm ERR! Fix the upstream dependency conflict\n```\n\nGrafana is on React 18. The @visx/visx library needs to work for data visualization.\n", "vague_prompt": "Can't install dependencies anymore. npm is complaining about React versions.", "ground_truth": {"root_cause": "@visx/visx 2.18.0 peer dependency too strict (only ^17.0.0)", "resolution_options": ["Upgrade to @visx/visx 3.x (supports React 18)", "Use npm overrides to force React 18", "Find alternative visx-compatible React 18 version"], "recommended": "Upgrade @visx/visx to 3.x, update breaking API usages", "migration_effort": "~15 files need API updates for visx 3.x"}, "scoring_criteria": {"correct_diagnosis": {"weight": 0.3, "levels": {"exact": "Identifies @visx version incompatibility", "partial": "Identifies peer dep issue"}}, "working_resolution": {"weight": 0.5, "levels": {"clean": "Upgrade to compatible version", "workaround": "Use overrides (may cause issues)", "broken": "Doesn't resolve"}}, "handles_migration": {"weight": 0.2, "levels": {"complete": "Updates breaking API usages", "partial": "Some usages updated"}}}, "difficulty": "medium", "tags": ["ci", "dependencies", "npm", "react"], "task_id": "grafana__grafana-dependency_conflict_resolution-107068e816d7", "category": "ci_cd", "created_at": "2025-12-29T18:16:22.945677+00:00"}
{"task_type": "test_gap_identification", "repo_name": "elastic/elasticsearch", "title": "Identify test gaps in snapshot restore", "scenario": "\nThe snapshot restore functionality has had several production bugs recently:\n1. Partial restore fails silently\n2. Cross-cluster restore corrupts index mappings\n3. Restore from cold storage times out\n\nCurrent test coverage appears adequate (85% line coverage), but bugs keep escaping.\n\nReview the test suite for snapshot restore and identify what's missing.\n\nKey files:\n- server/src/test/java/org/elasticsearch/snapshots/SnapshotRestoreTests.java\n- server/src/test/java/org/elasticsearch/snapshots/RestoreServiceTests.java\n", "vague_prompt": "Our snapshot tests have good coverage but bugs keep escaping. What are we missing?", "ground_truth": {"identified_gaps": ["No tests for partial restore with some shards unavailable", "Cross-cluster tests don't verify mapping compatibility", "No slow storage simulation in tests", "Missing concurrent restore + write tests", "No tests for restore interruption recovery"], "false_coverage": ["Tests cover happy path extensively", "Error paths have mocks that hide real behavior", "Integration tests run with minimal data"], "recommended_tests": ["Partial restore with simulated failures", "Cross-cluster with incompatible mappings", "Timeout behavior with throttled I/O", "Concurrent operations during restore"]}, "scoring_criteria": {"real_gaps_found": {"weight": 0.5, "levels": {"critical": "Finds gaps that match production bugs", "useful": "Finds real gaps but not production-relevant", "trivial": "Only finds minor coverage gaps"}}, "no_false_positives": {"weight": 0.2, "levels": {"clean": "All identified gaps are real", "some_noise": "Some false positives", "noisy": "Many false positives"}}, "actionable": {"weight": 0.3, "levels": {"complete": "Clear test specifications for each gap", "partial": "Some actionable recommendations", "vague": "Observations without clear actions"}}}, "difficulty": "expert", "tags": ["testing", "test-gaps", "coverage-analysis", "elasticsearch"], "task_id": "elastic__elasticsearch-test_gap_identification-a27c8f0d448a", "category": "test_evolution", "created_at": "2025-12-29T18:16:22.945680+00:00"}
{"task_type": "test_maintenance", "repo_name": "microsoft/vscode", "title": "Update tests after Settings UI refactor", "scenario": "\nThe Settings UI was refactored from class-based to functional React components.\n\n20 tests are now failing with:\n```\nTypeError: Cannot read property 'state' of undefined\n    at SettingsEditor.test.ts:45\n```\n\nThe tests were written for the old class-based API:\n```typescript\n// Old test pattern\nconst editor = new SettingsEditor();\neditor.setState({ query: 'font' });\nexpect(editor.getFilteredSettings()).toHaveLength(5);\n```\n\nUpdate the tests to work with the new functional component API while still validating the same behaviors.\n", "vague_prompt": "The Settings UI tests are broken after the refactor. Fix them.", "ground_truth": {"required_changes": ["Replace class instantiation with render()", "Use testing-library queries instead of state access", "Test through user interactions not internal state", "Mock hooks instead of class methods"], "behavior_to_preserve": ["Filtering settings by query", "Category navigation", "Setting modification", "Search highlighting"], "new_test_pattern": "\n// New test pattern\nrender(<SettingsEditor />);\nawait userEvent.type(screen.getByRole('searchbox'), 'font');\nexpect(screen.getAllByTestId('setting-item')).toHaveLength(5);\n"}, "scoring_criteria": {"tests_pass": {"weight": 0.4, "levels": {"all": "All 20 tests pass", "most": ">15 tests pass", "some": ">10 tests pass", "few": "<10 tests pass"}}, "same_coverage": {"weight": 0.3, "levels": {"equivalent": "Same behaviors tested", "reduced": "Some behaviors lost", "expanded": "More behaviors tested"}}, "idiomatic": {"weight": 0.3, "levels": {"modern": "Uses testing-library best practices", "functional": "Works but not idiomatic", "legacy": "Still testing implementation details"}}}, "difficulty": "medium", "tags": ["testing", "test-maintenance", "refactoring", "react"], "task_id": "microsoft__vscode-test_maintenance-93a777d5dcda", "category": "test_evolution", "created_at": "2025-12-29T18:16:22.945681+00:00"}
{"task_type": "doc_staleness_detection", "repo_name": "kubernetes/kubernetes", "title": "Audit kubectl documentation accuracy", "scenario": "\nUsers report that kubectl documentation is often out of date with actual behavior.\n\nReview the kubectl documentation against the current implementation for:\n- `kubectl apply` - particularly the strategic merge patch behavior\n- `kubectl rollout` - especially the new `rollout history` format\n- `kubectl debug` - which has had several recent additions\n\nIdentify any discrepancies between docs and actual behavior.\n", "vague_prompt": "Check if our kubectl docs are accurate", "ground_truth": {"discrepancies_found": ["kubectl apply --server-side not documented for namespace conflicts", "kubectl rollout history shows different columns than documented", "kubectl debug --copy-to not in stable docs despite being GA", "kubectl apply --prune behavior differs from docs for CRDs"], "docs_to_update": ["docs/reference/generated/kubectl/kubectl-commands.md", "docs/concepts/workloads/controllers/deployment.md"], "implementation_locations": ["staging/src/k8s.io/kubectl/pkg/cmd/apply/", "staging/src/k8s.io/kubectl/pkg/cmd/rollout/"]}, "scoring_criteria": {"real_discrepancies": {"weight": 0.5, "levels": {"significant": "Finds user-impacting discrepancies", "minor": "Finds cosmetic discrepancies", "none": "Misses real discrepancies"}}, "no_false_positives": {"weight": 0.2, "levels": {"accurate": "All flagged items are real issues", "mostly": "Some false positives"}}, "actionable_fixes": {"weight": 0.3, "levels": {"complete": "Specific doc updates proposed", "partial": "General areas identified"}}}, "difficulty": "hard", "tags": ["documentation", "kubectl", "accuracy-audit"], "task_id": "kubernetes__kubernetes-doc_staleness_detection-a9f8f4254a41", "category": "documentation", "created_at": "2025-12-29T18:16:22.945683+00:00"}
