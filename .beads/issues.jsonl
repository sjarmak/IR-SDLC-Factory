{"id":"IR-SDLC-Factory-04c","title":"Define IR impact telemetry capture","description":"Design telemetry capture to measure how IR quality impacts agent output. Key signals: 1) Retrieval accuracy (did agent find right files?), 2) Context utilization (how much retrieved context was used?), 3) Navigation efficiency (time/tokens spent searching vs implementing), 4) First-retrieval success (was first search attempt productive?). Should integrate with CodeContextBench observability module.","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:13:19.517943-05:00","updated_at":"2025-12-29T10:13:19.517943-05:00"}
{"id":"IR-SDLC-Factory-0t9","title":"Create enterprise codebase ground truth extraction","description":"Build pipeline to extract ground truth from enterprise codebases for IR evaluation. Sources: 1) Git history (commits that fixed issues), 2) PR file changes (reviewer-validated relevant files), 3) Issue-to-code links (files mentioned in issue resolution), 4) Test coverage changes (new tests added for bug fixes). Output: GroundTruth objects with CodeLocation lists per task.","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:13:26.664574-05:00","updated_at":"2025-12-29T10:13:26.664574-05:00"}
{"id":"IR-SDLC-Factory-1e8","title":"Create MCP-enabled Claude Code agent base","description":"Port the BaselineClaudeCodeAgent pattern from CodeContextBench. Key features: extends Harbor ClaudeCode, injects FORCE_AUTO_BACKGROUND_TASKS=1 and ENABLE_BACKGROUND_TASKS=1 for autonomous mode, uses --permission-mode acceptEdits --allowedTools for tool whitelisting. Supports optional MCP configuration via BASELINE_MCP_TYPE env var.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:07:50.809046-05:00","updated_at":"2025-12-29T10:12:52.766715-05:00","closed_at":"2025-12-29T10:12:52.766715-05:00"}
{"id":"IR-SDLC-Factory-43r","title":"Create observability module for execution tracing","description":"Build observability module for capturing agent execution traces: 1) Token count extraction from Claude output, 2) Tool usage tracking (which MCP tools called, frequency), 3) Timing metrics (query time, retrieval time, total time), 4) Cost calculation, 5) Run manifest generation. Reference CodeContextBench observability/ module.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-29T10:08:58.977045-05:00","updated_at":"2025-12-29T10:08:58.977045-05:00"}
{"id":"IR-SDLC-Factory-4b9","title":"Create documentation for IR benchmark setup","description":"Write comprehensive documentation: 1) ARCHITECTURE.md - system design and component overview, 2) DEVELOPMENT.md - setup instructions, environment config, running benchmarks, 3) MCP_INTEGRATION.md - how to add new MCP tools, endpoint configuration, 4) Update README.md with Harbor integration instructions. Reference CodeContextBench docs/ structure.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-29T10:09:04.754336-05:00","updated_at":"2025-12-29T10:09:04.754336-05:00"}
{"id":"IR-SDLC-Factory-4uw","title":"Define evaluation metrics for agent output quality","description":"Define metrics to measure how IR tool choice impacts coding agent output quality. Metrics should cover: 1) Task completion rate (binary success), 2) Code correctness (test pass rate), 3) Token efficiency (tokens used per task), 4) Time efficiency (wall clock), 5) Retrieval quality impact (correlation between IR metrics and agent success). Reference IRMetrics class for base IR metrics.","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:08:22.697518-05:00","updated_at":"2025-12-29T10:08:22.697518-05:00"}
{"id":"IR-SDLC-Factory-8ty","title":"Implement Harbor registry generator for IR benchmark","description":"Create tool to generate Harbor registry.json files for IR-SDLC benchmark tasks. Reference CodeContextBench/configs/harbor/registry.json format. Should support: task definitions with git_url and git_commit_id, dataset versioning, environment configuration (docker/daytona), and integration with the HarborTaskGenerator class.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-29T10:08:28.683836-05:00","updated_at":"2025-12-29T10:08:28.683836-05:00"}
{"id":"IR-SDLC-Factory-93x","title":"Add Daytona environment support for Harbor execution","description":"Add support for running agents in Daytona cloud environments (as used in CodeContextBench). Requires: DAYTONA_API_KEY env var, harbor run --env daytona flag support, integration with daytona_toolbox_api_client_async for container management. Reference: CodeContextBench harbor venv activation script and .env.local config.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:08:03.617173-05:00","updated_at":"2025-12-29T10:12:52.874021-05:00","closed_at":"2025-12-29T10:12:52.874021-05:00"}
{"id":"IR-SDLC-Factory-9rs","title":"Create generic MCP tool interface","description":"Design and implement a generic MCP tool interface that allows any MCP server to be used as an IR tool. This should: 1) Support HTTP MCP endpoints with configurable URL and auth headers, 2) Map MCP tool calls to IRToolInterface.retrieve() method, 3) Handle tool discovery and invocation, 4) Support both stdio and HTTP transport types. This enables testing with ANY MCP server (Deep Search, Sourcegraph, custom MCPs).","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-29T10:08:10.132288-05:00","updated_at":"2025-12-29T10:12:52.930717-05:00","closed_at":"2025-12-29T10:12:52.930717-05:00"}
{"id":"IR-SDLC-Factory-af0","title":"Build comparative analysis framework","description":"Create framework for A/B comparison of agent performance across IR tools. Should: 1) Run same tasks with baseline vs MCP-enabled agents, 2) Compute per-task and aggregate metrics, 3) Generate comparison reports (baseline vs Deep Search, baseline vs Sourcegraph full, etc.), 4) Statistical significance testing, 5) Visualization of results. Reference CodeContextBench runners/compare_results.py and aggregator.py.","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:08:35.090698-05:00","updated_at":"2025-12-29T10:08:35.090698-05:00"}
{"id":"IR-SDLC-Factory-e2n","title":"Design CodeContextBench dashboard integration schema","description":"Define the data schema for benchmark results that integrates with CodeContextBench dashboard. Must capture: 1) Task metadata (SDLC type, repo, difficulty), 2) IR-specific metrics (retrieval quality, context relevance), 3) Agent output metrics (success, token usage, time), 4) Comparative data for A/B analysis. Output should be compatible with CodeContextBench .dashboard_runs/ format and jobs/ structure.","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-29T10:13:06.933862-05:00","updated_at":"2025-12-29T10:25:01.044351-05:00","closed_at":"2025-12-29T10:25:01.044351-05:00"}
{"id":"IR-SDLC-Factory-ezs","title":"Create MCP config generator for agent setup","description":"Implement MCP config generator that creates .mcp.json files for different MCP endpoint configurations. Support: 1) HTTP MCP endpoints (type: http, url, headers), 2) Stdio MCP endpoints (type: stdio, command, args), 3) Environment variable templating for tokens/URLs, 4) Upload to container via Harbor environment. Reference CodeContextBench mcp_config patterns in agent setup() methods.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-29T10:08:47.529644-05:00","updated_at":"2025-12-29T10:12:52.973275-05:00","closed_at":"2025-12-29T10:12:52.973275-05:00"}
{"id":"IR-SDLC-Factory-ge3","title":"Create initial enterprise-scale benchmark dataset","description":"Generate an initial set of benchmark tasks from enterprise-scale repos for validation. Target: 25-50 tasks across 3-5 repos (e.g., kubernetes, pytorch, vscode). Use existing collect_large_repos.py for repo collection, then run task generation pipeline. Validate task quality with manual review sample. Reference CodeContextBench github_mined benchmark (25 PyTorch tasks).","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-29T10:08:40.971387-05:00","updated_at":"2025-12-29T10:08:40.971387-05:00"}
{"id":"IR-SDLC-Factory-h8a","title":"Create SWE-bench integration adapter","description":"Create adapter to run IR-SDLC benchmark tasks with SWE-bench dataset format. Reference CodeContextBench swe_bench_configs/ and benchmarks/swebench_pro/. Support: 1) Task conversion from SWE-bench to Harbor format, 2) Environment configuration (baseline, deepsearch_mcp, sourcegraph_mcp), 3) Integration with swebench-verified dataset, 4) Agent import path configuration.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-29T10:09:19.617806-05:00","updated_at":"2025-12-29T10:09:19.617806-05:00"}
{"id":"IR-SDLC-Factory-kb6","title":"Implement Deep Search MCP IR tool adapter","description":"Create an adapter that wraps the Sourcegraph Deep Search MCP endpoint (used in CodeContextBench) to work with the IRToolInterface. Should support the sg_deepsearch tool via HTTP MCP endpoint. Reference: CodeContextBench/agents/mcp_variants.py DeepSearchFocusedAgent implementation.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:07:37.591244-05:00","updated_at":"2025-12-29T10:12:52.66793-05:00","closed_at":"2025-12-29T10:12:52.66793-05:00"}
{"id":"IR-SDLC-Factory-ky9","title":"Add test infrastructure for benchmark evaluation","description":"Create test infrastructure for validating benchmark execution: 1) Unit tests for IR tool adapters, 2) Integration tests for Harbor task generation, 3) End-to-end smoke test with hello-world task, 4) Mock MCP server for testing without real endpoints. Reference CodeContextBench tests/ directory structure.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-29T10:08:53.047654-05:00","updated_at":"2025-12-29T10:08:53.047654-05:00"}
{"id":"IR-SDLC-Factory-mdj","title":"Design MCP tool adapter interface for IR benchmark","description":"Design the MCPToolAdapter class that wraps MCP servers to implement IRToolInterface. Key design decisions: 1) How to map MCP tool invocations to retrieve() calls, 2) How to handle different MCP transport types (HTTP vs stdio), 3) How to configure tool-specific parameters (query formatting, result parsing), 4) How to support both single-tool MCPs (Deep Search only) and multi-tool MCPs (full Sourcegraph). This is a foundational architecture task that other MCP adapters will build on.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:07:31.417273-05:00","updated_at":"2025-12-29T10:12:47.944928-05:00","closed_at":"2025-12-29T10:12:47.944928-05:00"}
{"id":"IR-SDLC-Factory-mzi","title":"Validate benchmark difficulty calibration","description":"Validate that generated benchmarks have appropriate difficulty distribution. Run pilot tasks with baseline agent to establish: 1) Expected success rates per difficulty level, 2) Correlation between repo size/complexity and task difficulty, 3) SDLC task type difficulty patterns. Adjust difficulty estimation heuristics based on results.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-29T10:13:39.185902-05:00","updated_at":"2025-12-29T10:13:39.185902-05:00"}
{"id":"IR-SDLC-Factory-v7c","title":"Create LLM judge for agent output quality","description":"Implement LLM-based judge to evaluate agent output quality beyond test pass/fail. Criteria: 1) Code correctness (does solution address the issue?), 2) Code quality (idiomatic, maintainable?), 3) Completeness (all requirements met?), 4) Efficiency (no unnecessary changes?). Should output structured scores compatible with CodeContextBench llm_judge_results.json format.","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:13:33.201154-05:00","updated_at":"2025-12-29T10:13:33.201154-05:00"}
{"id":"IR-SDLC-Factory-vj2","title":"Create SDLC benchmark task generation pipeline","description":"Build the pipeline to generate benchmark tasks from real-world sources: 1) GitHub issue mining (bug reports, PRs), 2) Ground truth extraction from fix commits, 3) Task difficulty estimation based on repo size/complexity, 4) Support for all 10 SDLC task types. Use the SDLCTaskGenerator classes in task_types.py. Output: JSONL dataset compatible with Harbor.","status":"in_progress","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:08:16.662145-05:00","updated_at":"2025-12-29T10:28:47.616249-05:00"}
{"id":"IR-SDLC-Factory-x3n","title":"Implement MCP agent variants for A/B testing","description":"Port the 4 MCP agent variants from CodeContextBench: 1) StrategicDeepSearchAgent - uses Deep Search at strategic moments, 2) DeepSearchFocusedAgent - aggressive Deep Search usage, 3) MCPNonDeepSearchAgent - keyword/NLS only, 4) FullToolkitAgent - neutral prompting with all tools. Each variant has specific system prompts and CLAUDE.md files.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:07:57.50058-05:00","updated_at":"2025-12-29T10:12:52.823741-05:00","closed_at":"2025-12-29T10:12:52.823741-05:00"}
{"id":"IR-SDLC-Factory-xe4","title":"Implement Sourcegraph full MCP IR tool adapter","description":"Create an adapter for full Sourcegraph MCP with all tools (sg_deepsearch, sg_keyword_search, sg_nls_search, sg_read_file). Maps to the FullToolkitAgent pattern from CodeContextBench. MCP config: {type: http, url: SOURCEGRAPH_URL/.api/mcp/v1}.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:07:43.754086-05:00","updated_at":"2025-12-29T10:12:52.717146-05:00","closed_at":"2025-12-29T10:12:52.717146-05:00"}
{"id":"IR-SDLC-Factory-yb2","title":"Create benchmark exporter for CodeContextBench","description":"Build exporter that outputs IR-SDLC benchmarks in format consumable by CodeContextBench. Tasks should: 1) Generate Harbor-compatible task directories, 2) Include SDLC-specific metadata in task.toml, 3) Create Dockerfiles that clone enterprise repos at specific commits, 4) Generate test.sh scripts that capture IR impact metrics. Reference CodeContextBench benchmarks/ directory structure.","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-29T10:13:13.43834-05:00","updated_at":"2025-12-29T10:13:13.43834-05:00"}
{"id":"IR-SDLC-Factory-zyn","title":"Run baseline vs MCP ablation study","description":"Execute first ablation study comparing agent performance: 1) Baseline (no MCP) vs Deep Search MCP, 2) Baseline vs Full Sourcegraph MCP, 3) Deep Search only vs Full toolkit. Run on initial benchmark dataset (25-50 tasks). Collect metrics: success rate, token efficiency, time. Generate comparison report. This validates the benchmark framework end-to-end.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-29T10:09:25.483595-05:00","updated_at":"2025-12-29T10:09:25.483595-05:00"}
